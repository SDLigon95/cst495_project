{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Spark\n",
    "\n",
    "Whenever we work in Spark the first thing we need is the spark contect (sc).  We are going to use the module `findspark` to get access to the spark context.  First we need to install the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): findspark in /home/nigel/anaconda2/lib/python2.7/site-packages\n",
      "\u001b[33mYou are using pip version 8.0.2, however version 8.1.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we specify the path to spark - which for us is on the local VM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init(os.getenv('HOME') + '/spark-1.6.0-bin-hadoop2.6')\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import pyspark and get the spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f0d4831f510>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "try: \n",
    "    print(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext()\n",
    "    print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an RDD\n",
    "\n",
    "From the Spark documentation:\n",
    "\n",
    "_\"A Resilient Distributed Dataset (RDD), the basic abstraction in Spark, represents an immutable, partitioned collection of elements that can be operated on in parallel.\"_\n",
    "\n",
    "_\"Parallelized collections are created by calling SparkContextâ€™s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel.\"_ \n",
    "\n",
    "For example, here is how to create a parallelized collection holding the numbers 1 to 5:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "\n",
    "print(distData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD exists in the Spark Context which may or may not be in the notebook kernel.\n",
    "\n",
    "We apply transformations and actions to the RDD. The Spark driver will manage the RDD and when needed execute operations in parallel, for example to add up elements of list.\n",
    "\n",
    "Spark is heavily functional (built in Scala).  For example, map, reduce and filter operations are supported - these functions take functions or lambda functions as arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics - transformations and actions\n",
    "\n",
    "The RDD is not loaded in memory - it is just a pointer to the file.  Spark allows us to apply transformations to the RDD.  The transformations are not computed immediately - Spark is intentionally lazy.  Nothing is computed until we execute an action. Actions require the Spark driver to execute the tasks which run on separate nodes in the Spark cluster.  Each node executes the transformations and actions and returns the results to the driver.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "distData = sc.parallelize(data) \\\n",
    "                .filter(lambda x : x > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData = sc.parallelize(data) \\\n",
    "                .filter(lambda x : x > 3) \\\n",
    "                .map(lambda x : x ** 2)\n",
    "type(distData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'> `filter()` applies a logical expression to each element of the RDD - those that return TRUE are kept in the output RDD.  \n",
    "`map()` applies a transformation to each element of the RDD - here we transform each element by outputing the x-squared.\n",
    "\n",
    "Actions force the collection of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'int'>\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "from operator import add, mul \n",
    "\n",
    "distData = sc.parallelize(data) \\\n",
    "                .filter(lambda x : x > 3) \\\n",
    "                .map(lambda x : x ** 2) \\\n",
    "                .reduce(add)\n",
    "\n",
    "print(type(distData))\n",
    "\n",
    "print(distData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'>`reduce()` merges the values using an associative reduce function.  For example with the `add` function if we had values [1,2,3,4] then reduce first computes 1+2=3, then adds the results to the next value 3+3=6, and then adds the result to the next value 6+4=10 until the list has been processed.  Associative means not dependent on the order of the list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>Run the examples above.  Compare the `type` of object before and after the reduce action is applied.  Why does it change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External Data Sources\n",
    "\n",
    "We can also create RDDs from external data sources such as Hadoop, Amazon S3 and files. Here we will create a text file RDD.  Note that we must use absolute paths since this code is pushed onto the Spark cluster - it is not run in the context of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapPartitionsRDD[6] at textFile at NativeMethodAccessorImpl.java:-2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[u'cycling bicycle mtb bike fixie gloss carbon fiber riser bar handlebar,description feature easy to use made of high quality carbon fiber with the special design can save for a long time the carbon fiber handlebar is made of high quality carbon fiber so that you can use it relieved this quick disassembling carbon fiber handlebar is easy to use and one of the best gifts to your friends specification material carbon fiber color black handlebar clamp diameter mm length package included x cycling carbon fiber rise',\n",
       " u'bicycle rims x red speed internal hub wheel set beach cruiser bike,clyde james cycles x speed internal hub red wheel set most orders ship within days after receiving payment threw paypal the shipping rates listed are only for residential or commercial destinations in the continental united states please send a message to quote to other destinations we only ship the item to paypal verified address that is sent to us at the time of checkout please verify the correct shipping address in your paypal account before making your payme']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile(os.getcwd()+'/data/bike-items-clean.txt')\n",
    "print(rdd)\n",
    "\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching RDDs\n",
    "\n",
    "In the previous example the `take()` action forces the Spark driver to create the RDD - reading from disk to memory.\n",
    "\n",
    "If we are going to use an RDD interactively we can cache the RDD.  This means we only need to apply actions after the point of caching - however if the cluster fails Spark still knows how to recreate the RDD.\n",
    "\n",
    "In this example we cache the rdd and then extract different numbers of records.  Note the quicker execution of subsequent calls to the `take()` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 98.4 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "rdd = sc.textFile(os.getcwd()+'/data/bike-items-clean.txt')\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 220 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "rdd = sc.textFile(os.getcwd()+'/data/bike-items-clean.txt')\n",
    "rdd.show()\n",
    "rdd.cache()\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 68.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Words\n",
    "\n",
    "To illustrate RDD basics, consider the simple program below which counts the number of words in the text file rdd we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733764\n"
     ]
    }
   ],
   "source": [
    "words_per_line = rdd.map(lambda s: len(s.split())).filter(lambda x : x > 2)\n",
    "\n",
    "total_words = words_per_line.reduce(lambda x,y : x+y)\n",
    "\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'>  To reiterate - `words_per_line` applies a transformation to the rdd.  No computation happens.  \n",
    "Work starts when we apply an action - such as `reduce()`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting RDD's\n",
    "\n",
    "We can inspect the transformations applied to the RDD using the `toDebugString()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) PythonRDD[708] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[529] at textFile at null:-1 []\n",
      " |  /home/otter/data-science-for-search/data/bike-items-clean.txt HadoopRDD[528] at textFile at null:-1 []\n"
     ]
    }
   ],
   "source": [
    "print(words_per_line.toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the DAG and the jobs on the cluster [here](http://localhost:4040)\n",
    "\n",
    "![Spark DAG](resources/spark-dag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency in Spark\n",
    "\n",
    "Many ways to do this - some are more efficient than others.\n",
    "\n",
    "Method 1 - use map and reduce operations\n",
    "\n",
    "* map() - split each line on comma - extract item title, discard item description\n",
    "* flatMap() - split item title on whitespace - vector of words in title\n",
    "* map() - transform to key, value tuple (word, 1)\n",
    "* reduceByKey() - reduce by key adding the values - requires a shuffle\n",
    "* collectAsMap()- return results as a dictionary of (word, count) pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_info_outline_black_24dp_2x.png' align='left'>`reduceByKey` merges the values for each key using an associative reduce function.  For example if a key had values [1,2,3,4] then reduce by key first computes 1+2=3, then adds the results to the next value 3+3=6, and then adds the result to the next value 6+4=10 until the list has been processed.  Associative means not dependent on the order of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4534\n"
     ]
    }
   ],
   "source": [
    "terms1 = rdd.map(lambda s : s.split(',')[0]) \\\n",
    "            .flatMap(lambda s : s.split()) \\\n",
    "            .map(lambda w : (w, 1)) \\\n",
    "            .reduceByKey(lambda x,y : x+y) \\\n",
    "            .collectAsMap()\n",
    "\n",
    "\n",
    "\n",
    "print(terms1['bike'])\n",
    "\n",
    "#?rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2 - use Spark API to optimise\n",
    "\n",
    "* map - split each line on comma - extract item title, discard item description\n",
    "* flatMap - split item title on whitespace - vector of words in titles\n",
    "* countByValue() - return the count of each unique value in the RDD as a dictionary of (value, count) pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4534\n"
     ]
    }
   ],
   "source": [
    "terms2 = rdd.map(lambda s : s.split(',')[0]) \\\n",
    "            .flatMap(lambda s : s.split()) \\\n",
    "            .countByValue()\n",
    "        \n",
    "print(terms2['bike'])\n",
    "\n",
    "#?rdd.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toDebugString() starts to get more interesting with bigger pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) PythonRDD[719] at RDD at PythonRDD.scala:43 []\n",
      " |  MapPartitionsRDD[718] at mapPartitions at PythonRDD.scala:374 []\n",
      " |  ShuffledRDD[717] at partitionBy at null:-1 []\n",
      " +-(1) PairwiseRDD[716] at reduceByKey at <ipython-input-126-3254cf225807>:1 []\n",
      "    |  PythonRDD[715] at reduceByKey at <ipython-input-126-3254cf225807>:1 []\n",
      "    |  MapPartitionsRDD[529] at textFile at null:-1 []\n",
      "    |  /home/otter/data-science-for-search/data/bike-items-clean.txt HadoopRDD[528] at textFile at null:-1 []\n"
     ]
    }
   ],
   "source": [
    "print(rdd.flatMap(lambda s : s.split()) \\\n",
    "            .map(lambda w : (w, 1)) \\\n",
    "            .reduceByKey(lambda x,y : x+y).toDebugString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>Experiment by breaking down the pipelines and inspecting the contents of the RDDs using `take()`.  \n",
    "Make sure you understand the different between `map()` and `flatMap()` before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark DataFrames API\n",
    "\n",
    "Spark DataFrames API is becoming very popular abstraction on top of RDDs - largely due to the familiarity of the idea of a dataframe inherited from R and Pandas. Let's take a look!\n",
    "\n",
    "CSV parsing is surprisingly complex (we dodged it above by using a clean text file).  The Spark DataFrames API provides a good CSV parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          item_title|\n",
      "+---+--------------------+\n",
      "|  2|ZIPP VUKA CARBON ...|\n",
      "|  3|Cycling Bicycle M...|\n",
      "|  4|BICYCLE RIMS 26\"x...|\n",
      "|  5|Mavic Crossride 2...|\n",
      "|  7|ROTOR QXL Aero Ov...|\n",
      "|  8|Yakima 4 pack SKS...|\n",
      "|  9|Sram Force Carbon...|\n",
      "| 10|THE ORIGINAL SQUI...|\n",
      "| 11|BV Bike Rear Sadd...|\n",
      "| 12|HELIX BMX ROUND D...|\n",
      "| 13|Waterproof Bicycl...|\n",
      "| 14|Brand New CycleOp...|\n",
      "| 15|Planet Bike LED S...|\n",
      "| 16|Bike Bicycle Head...|\n",
      "| 17|New Helmet Teenag...|\n",
      "| 18|2 Pcs Bike Roller...|\n",
      "| 19|FSA BICYCLE COMPR...|\n",
      "| 20|Kenda Tube 26 X1....|\n",
      "| 21|Bicycle Lock Set ...|\n",
      "| 22|NEW DT Swiss 350 ...|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "        .options(header='false', inferSchema='true') \\\n",
    "        .load(os.getcwd() + '/data/bike-item-titles.txt') \\\n",
    "        .selectExpr(\"C0 as id\",\"C1 as item_title\")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,IntegerType,true),StructField(item_title,StringType,true)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames API has functional model that can be applied to data frame objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['id'] >=5).filter(df['id'] <= 10).count()\n",
    "\n",
    "#df.filter(df['id'] >=5).filter(df['id'] <= 10).collect()\n",
    "\n",
    "#df.filter(df['id'] >=5).filter(df['id'] <= 10).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also has SQL interface, first we register the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'bikeitems']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.registerDataFrameAsTable(df,'bikeitems')\n",
    "sqlContext.tableNames()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute SQL against the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          item_title|\n",
      "+---+--------------------+\n",
      "|  5|Mavic Crossride 2...|\n",
      "|  7|ROTOR QXL Aero Ov...|\n",
      "|  8|Yakima 4 pack SKS...|\n",
      "|  9|Sram Force Carbon...|\n",
      "| 10|THE ORIGINAL SQUI...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"select id, item_title from bikeitems where id between 5 and 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert back and forth between RDDs and DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=2, item_title=u'ZIPP VUKA CARBON AERO BASE BAR AND EXTENSIONS COMPLETE TRIATHLON TT TRI CYCLING'),\n",
       " Row(id=3, item_title=u'Cycling Bicycle MTB Bike Fixie Gloss 3K Carbon Fiber Riser Bar Handlebar 31.8mm'),\n",
       " Row(id=4, item_title=u'BICYCLE RIMS 26\"x 50MM RED 3 SPEED INTERNAL HUB WHEEL SET BEACH CRUISER BIKE'),\n",
       " Row(id=5, item_title=u'Mavic Crossride 26\" Mountain bike wheels and WTB Weirwolf Tires'),\n",
       " Row(id=7, item_title=u'ROTOR QXL Aero Oval Road Chainring BCD110x5 53t'),\n",
       " Row(id=8, item_title=u'Yakima 4 pack SKS lock cores & 2 keys - A142 - roof rack locking cylinders'),\n",
       " Row(id=9, item_title=u'Sram Force Carbon Crank Gxp 110 Bcd No Chainrings 175 mm (2700)'),\n",
       " Row(id=10, item_title=u'THE ORIGINAL SQUIRT LONG LASTING DRY CHAIN BICYCLE LUBE WAX BASED'),\n",
       " Row(id=11, item_title=u'BV Bike Rear Saddle Bag Cycling Seat Post Pouch Bicycle Tail Storage NEW SB1-L'),\n",
       " Row(id=12, item_title=u'HELIX BMX ROUND DROPOUT SAVERS -FITS NEARLY ALL FRAMES -Fits 3/8\" AND 10mm Axles')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = df.rdd\n",
    "df2 = rdd.toDF()\n",
    "\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index\n",
    "\n",
    "Let's imagine we want to use Spark to compute and inverted index for our set of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=2, item_title=u'ZIPP VUKA CARBON AERO BASE BAR AND EXTENSIONS COMPLETE TRIATHLON TT TRI CYCLING'),\n",
       " Row(id=3, item_title=u'Cycling Bicycle MTB Bike Fixie Gloss 3K Carbon Fiber Riser Bar Handlebar 31.8mm'),\n",
       " Row(id=4, item_title=u'BICYCLE RIMS 26\"x 50MM RED 3 SPEED INTERNAL HUB WHEEL SET BEACH CRUISER BIKE'),\n",
       " Row(id=5, item_title=u'Mavic Crossride 26\" Mountain bike wheels and WTB Weirwolf Tires'),\n",
       " Row(id=7, item_title=u'ROTOR QXL Aero Oval Road Chainring BCD110x5 53t'),\n",
       " Row(id=8, item_title=u'Yakima 4 pack SKS lock cores & 2 keys - A142 - roof rack locking cylinders'),\n",
       " Row(id=9, item_title=u'Sram Force Carbon Crank Gxp 110 Bcd No Chainrings 175 mm (2700)'),\n",
       " Row(id=10, item_title=u'THE ORIGINAL SQUIRT LONG LASTING DRY CHAIN BICYCLE LUBE WAX BASED'),\n",
       " Row(id=11, item_title=u'BV Bike Rear Saddle Bag Cycling Seat Post Pouch Bicycle Tail Storage NEW SB1-L'),\n",
       " Row(id=12, item_title=u'HELIX BMX ROUND DROPOUT SAVERS -FITS NEARLY ALL FRAMES -Fits 3/8\" AND 10mm Axles')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ZIPP', 2),\n",
       " (u'VUKA', 2),\n",
       " (u'CARBON', 2),\n",
       " (u'AERO', 2),\n",
       " (u'BASE', 2),\n",
       " (u'BAR', 2),\n",
       " (u'AND', 2),\n",
       " (u'EXTENSIONS', 2),\n",
       " (u'COMPLETE', 2),\n",
       " (u'TRIATHLON', 2),\n",
       " (u'TT', 2),\n",
       " (u'TRI', 2),\n",
       " (u'CYCLING', 2),\n",
       " (u'Cycling', 3),\n",
       " (u'Bicycle', 3),\n",
       " (u'MTB', 3),\n",
       " (u'Bike', 3),\n",
       " (u'Fixie', 3),\n",
       " (u'Gloss', 3),\n",
       " (u'3K', 3)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = rdd.flatMap(lambda row : [ ( word, row[0]) for word in row[1].split(' ') ] ) \n",
    "index.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'', <pyspark.resultiterable.ResultIterable at 0x7f65152915d0>),\n",
       " (u'Powerlock-New', <pyspark.resultiterable.ResultIterable at 0x7f6515291ad0>),\n",
       " (u'BLACK/SILVER', <pyspark.resultiterable.ResultIterable at 0x7f6515291f90>),\n",
       " (u'SecurityIng', <pyspark.resultiterable.ResultIterable at 0x7f6515291cd0>),\n",
       " (u'SporstWear', <pyspark.resultiterable.ResultIterable at 0x7f6515291a90>),\n",
       " (u'(28.6)', <pyspark.resultiterable.ResultIterable at 0x7f65152919d0>),\n",
       " (u'S-5', <pyspark.resultiterable.ResultIterable at 0x7f6515291290>),\n",
       " (u'Interloc', <pyspark.resultiterable.ResultIterable at 0x7f6515291550>),\n",
       " (u'S-2', <pyspark.resultiterable.ResultIterable at 0x7f6515291fd0>),\n",
       " (u'yellow', <pyspark.resultiterable.ResultIterable at 0x7f6515291410>)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = rdd.flatMap(lambda row : [ (word,  row[0]) for word in row[1].split(' ') ] ) \\\n",
    "            .groupByKey()\n",
    "index.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Unicycle', [2138, 3748, 7232, 8777])]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = rdd.flatMap(lambda row : [ (word,  row[0]) for word in row[1].split(' ') ] ) \\\n",
    "            .groupByKey() \\\n",
    "            .map(lambda x : (x[0], list(x[1])))\n",
    "index.filter(lambda x : x[0] == 'Unicycle').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = rdd.flatMap(lambda row : [ (word,  row[0]) for word in row[1].split(' ') ] ) \\\n",
    "            .groupByKey() \\\n",
    "            .map(lambda x : (x[0], list(x[1]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Unicycle', [2138, 3748, 7232, 8777])]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.filter(lambda x : x[0] == 'Unicycle').take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='files/resources/ic_assignment_black_24dp_2x.png' align='left'>The index has upper and lower case tokens, for example 'Unicycle' and 'unicycle'.  \n",
    "Can you modify the index to normalise the tokens to lowercase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very quick overview - however you are in a great spot to now try out more of the great examples from the [Spark documentation](http://spark.apache.org/docs/latest/)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
